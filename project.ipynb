{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "这是 Week 6 的 autograd 的一个实现。\n",
    "## 为什么要自动求导？\n",
    "当然是因为用起来方便啊\n",
    "## 总体思路：\n",
    " * 定义好计算图（静态图）\n",
    " * 根据已经定义的图，进行前传（foward）得到损失（loss）。\n",
    " * 根据已经定义的图，进行反向传播（backward），使得梯度（gradient）反向流动。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import math\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注册节点和会使用到的信息所使用的变量（dict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数\n",
    "```get_by_name```: 按照变量名称获取 ```variables``` 中的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_name(name):\n",
    "    if 'variables' in globals():\n",
    "        try:\n",
    "            return variables[name]\n",
    "        except:\n",
    "            raise Exception('Variable with given name: %s is not in variables '%name)\n",
    "    else:\n",
    "        raise Exception('\"variables\" is not defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable 类\n",
    "用到的所有参与计算的变量\n",
    "\n",
    "使用 '-' 来分隔每个变量下的属性\n",
    "### 参数初始化\n",
    "```initializer```\n",
    "\n",
    "随机，全零，或者 MSRA 方式的初始化：一个均值为 0 方差为 $\\frac{2}{n}$ 的高斯分布。\n",
    "\n",
    "每个 ```Variable``` 都有对应的 ```child``` 和 ```parent```。\n",
    "\n",
    "每个 ```Variable``` 都有 ```apply_gradient``` 方法，用于将计算得出的梯度 ```grad``` 应用到对应 ```data``` (weight) 上面\n",
    "\n",
    "每个 ```Variable``` 都有 ```learnable``` 和 ```grad``` 属性，代表是否是可学习变量和是否需要求导。同时，```waiting_for_backprop``` 代表该变量当前是否需要反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable.py\n",
    "\n",
    "class Variable(object):\n",
    "    initial_method = 'MSRA'\n",
    "    method = 'SGD'\n",
    "\n",
    "    def __init__(self, shape=list, name=str, scope='', grad=True, learnable=False, init='MSRA'):\n",
    "        if scope != '':\n",
    "            self.scope = scope if scope[-1] == '-' else scope + '-'\n",
    "            self.name = self.scope + name\n",
    "        else:\n",
    "            self.name = name\n",
    "            self.scope = scope\n",
    "\n",
    "        if self.name in variables:\n",
    "            raise Exception('Variable with given name: %s exists!' % self.name)\n",
    "        else:\n",
    "            variables[self.name] = self\n",
    "\n",
    "        for idx, v in enumerate(shape):\n",
    "            if not isinstance(v, int):\n",
    "                if isinstance(v, float):\n",
    "                    shape[idx] = int(v)\n",
    "                    continue\n",
    "                raise Exception(\"Variable name: %s shape is not list of int\"%self.name)\n",
    "\n",
    "        self.shape = shape\n",
    "        self.data = self.initializer(shape, self.initial_method)\n",
    "        self.child = []\n",
    "        self.parent = []\n",
    "\n",
    "        if grad:\n",
    "            self.grad = np.zeros(self.shape)\n",
    "            self.waiting_for_backprop = True\n",
    "            self.learnable = learnable\n",
    "\n",
    "    def eval(self):\n",
    "        for operator in self.parent:\n",
    "            variables[operator].forward()\n",
    "        self.waiting_for_backprop = True\n",
    "        return self.data\n",
    "\n",
    "    def backward(self):\n",
    "        if self.waiting_for_backprop:\n",
    "            for operator in self.child:\n",
    "                variables[operator].backward()\n",
    "            self.waiting_for_backprop = False\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return self.grad\n",
    "\n",
    "    def apply_gradient(self, learning_rate=float, decay_rate=float, batch_size=1):\n",
    "        self.data *= (1 - decay_rate)\n",
    "        if self.method == 'SGD':\n",
    "            learning_rate = learning_rate\n",
    "            self.data -= (learning_rate * self.grad / batch_size)\n",
    "            self.grad *= 0\n",
    "\n",
    "        elif self.method == 'Momentum':\n",
    "            self.mtmp = self.momentum * self.mtmp + self.grad / batch_size\n",
    "            self.data -= learning_rate * self.mtmp\n",
    "            self.grad *= 0\n",
    "\n",
    "        else:\n",
    "            raise Exception('No apply_gradient method: %s'%self.method)\n",
    "\n",
    "    def set_method_sgd(self):\n",
    "        self.method = 'SGD'\n",
    "\n",
    "    def set_method_momentum(self, momentum=0.9):\n",
    "        self.method = 'Momentum'\n",
    "        self.momentum = momentum\n",
    "        self.mtmp = np.zeros(self.grad.shape)\n",
    "\n",
    "    # this function initialize an array with given shape and method.\n",
    "    def initializer(self, shape, method):\n",
    "        # random initialization\n",
    "        if method == 'const':\n",
    "            return np.random.standard_normal(shape) / 100\n",
    "        \n",
    "        # initialize with all zero\n",
    "        if method == 'None':\n",
    "            return np.zeros(shape)\n",
    "        # MSRA method\n",
    "        if method == 'MSRA':\n",
    "            weights_scale = math.sqrt(reduce(lambda x, y: x * y, shape) / shape[-1])\n",
    "            return np.random.standard_normal(shape) / weights_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator 基类\n",
    "用于定义变量间的操作。\n",
    "\n",
    "初始化函数中的 ```input_variable``` 和 ```output_variable``` 用于初始化该 ```Operator``` 对应的边的父节点和子节点。\n",
    "\n",
    "留出 ```forward()``` 和 ```backward()``` 方法，等待重载。\n",
    "\n",
    "具有 ```waiting_for_forward``` 属性，代表该节点此时是否正在等待前向传播。\n",
    "\n",
    "前传时，首先确定父节点（变量）已求值完毕，再对自身求值，最后 unset 自身 ```waiting_for_forward``` 属性。\n",
    "\n",
    "反向传播时，首先确定该节点是否正在等待前传，若是则跳过；检查子节点（变量）是否正在等待被微分并进行微分，再对自身求值，最后 set 自身 ```waiting_for_forward``` 属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operator(object):\n",
    "    def __init__(self, name, input_variables, output_variables):\n",
    "        \n",
    "        # init input check\n",
    "        if name in variables.keys():\n",
    "            raise Exception(\"Operator %s exists !\"%name)\n",
    "        \n",
    "        if not isinstance(input_variables, Variable) and not isinstance(input_variables[0], Variable):\n",
    "            raise Exception(\"Operator %s 's input_variables is not instance (or list) of Variable!\")\n",
    "\n",
    "        if not isinstance(output_variables, Variable) and not isinstance(output_variables[0], Variable):\n",
    "            raise Exception(\"Operator %s 's output_variables is not instance (or list) of Variable!\")\n",
    "\n",
    "        self.name = name\n",
    "        variables[self.name] = self\n",
    "        \n",
    "        self.child = []\n",
    "        self.parent =[]\n",
    "        \n",
    "        # register for input Variable's child node and output Variable's parents' node\n",
    "        register_graph(input_variables, output_variables, self)\n",
    "\n",
    "        self.waiting_for_forward = True\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "\n",
    "规定的 ```kernel_shape``` 的格式为 【卷积核边长（假设卷积核形状为正方形），卷积核边长，输入图片通道数量，输出图片通道数量】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(Operator):\n",
    "\n",
    "    def __init__(self, kernel_shape=list, input_variable=Variable, name=str, stride=1, padding='SAME'):\n",
    "        # kernel_shape = [ksize, ksize, input_channels, output_channels]\n",
    "        for i in kernel_shape:\n",
    "            if not isinstance(i, int):\n",
    "                raise Exception(\"Operator Conv2D name: %s kernel shape is not list of int\" % self.name)\n",
    "\n",
    "        if not isinstance(input_variable, Variable):\n",
    "            raise Exception(\"Operator Conv2D name: %s's input_variable is not instance of Variable\" % name)\n",
    "\n",
    "        if len(input_variable.shape)!=4:\n",
    "            raise Exception(\"Operator Conv2D name: %s's input_variable's shape != 4d Variable!\" % name)\n",
    "\n",
    "        self.ksize = kernel_shape[0]\n",
    "        self.stride = stride\n",
    "        self.output_num = kernel_shape[-1]\n",
    "        self.padding = padding\n",
    "        self.col_image = []\n",
    "\n",
    "        self.weights = Variable(kernel_shape, scope=name, name='weights',learnable=True)\n",
    "        self.bias = Variable([self.output_num], scope=name, name='bias', learnable=True)\n",
    "        self.batch_size = input_variable.shape[0]\n",
    "\n",
    "        if self.padding == 'SAME':\n",
    "            _output_shape = [self.batch_size, input_variable.shape[1] / stride, input_variable.shape[2] / stride,\n",
    "                             self.output_num]\n",
    "        if self.padding == 'VALID':\n",
    "            _output_shape = [self.batch_size, (input_variable.shape[1] - self.ksize + 1) / stride,\n",
    "                             (input_variable.shape[2] - self.ksize + 1) / stride, self.output_num]\n",
    "\n",
    "        self.output_variables = Variable(_output_shape, name='out', scope=name)  # .name\n",
    "        self.input_variables = input_variable\n",
    "        Operator.__init__(self, name, self.input_variables, self.output_variables)\n",
    "\n",
    "    def forward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            for parent in self.parent:\n",
    "                variables[parent].eval()\n",
    "            self._conv(self.input_variables, self.output_variables, self.weights.data, self.bias.data)\n",
    "            self.waiting_for_forward = False\n",
    "            return\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def backward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            pass\n",
    "        else:\n",
    "            for child in self.child:\n",
    "                variables[child].backward()\n",
    "            self._deconv(self.input_variables, self.output_variables, self.weights, self.bias)\n",
    "            self.waiting_for_forward = True\n",
    "            return\n",
    "\n",
    "    def _deconv(self, input=Variable, output=Variable, weights=Variable, bias=Variable):\n",
    "        col_eta = np.reshape(output.grad, [self.batch_size, -1, self.output_num])\n",
    "        for i in range(self.batch_size):\n",
    "            weights.grad += np.dot(self.col_image[i].T, col_eta[i]).reshape(self.weights.shape)\n",
    "        bias.grad += np.sum(col_eta, axis=(0, 1))\n",
    "\n",
    "        # deconv of padded eta with flippd kernel to get next_eta\n",
    "        if self.padding == 'VALID':\n",
    "            pad_eta = np.pad(output.grad, (\n",
    "                (0, 0), (self.ksize - 1, self.ksize - 1), (self.ksize - 1, self.ksize - 1), (0, 0)),\n",
    "                             'constant', constant_values=0)\n",
    "\n",
    "        if self.padding == 'SAME':\n",
    "            pad_eta = np.pad(output.grad, (\n",
    "                (0, 0), (self.ksize / 2, self.ksize / 2), (self.ksize / 2, self.ksize / 2), (0, 0)),\n",
    "                             'constant', constant_values=0)\n",
    "\n",
    "        col_pad_eta = np.array([im2col(pad_eta[i][np.newaxis, :], self.ksize, self.stride) for i in range(self.batch_size)])\n",
    "        flip_weights = np.flipud(np.fliplr(weights.data))\n",
    "        flip_weights = flip_weights.swapaxes(2,3)\n",
    "        col_flip_weights = flip_weights.reshape([-1, weights.shape[2]])\n",
    "        next_eta = np.dot(col_pad_eta, col_flip_weights)\n",
    "        next_eta = np.reshape(next_eta, input.shape)\n",
    "        input.grad = next_eta\n",
    "        return\n",
    "\n",
    "    def _conv(self, input=Variable, output=Variable, weights=np.ndarray, bias=np.ndarray):\n",
    "        # reshape weights to col\n",
    "        col_weights = weights.reshape(-1, self.output_num)\n",
    "\n",
    "        # padding input_img according to method\n",
    "        if self.padding == 'SAME':\n",
    "            batch_img = np.pad(input.data, (\n",
    "                (0, 0), (self.ksize / 2, self.ksize / 2), (self.ksize / 2, self.ksize / 2), (0, 0)),\n",
    "                               'constant', constant_values=0)\n",
    "        else:\n",
    "            batch_img = input.data\n",
    "\n",
    "        # malloc tmp output_data\n",
    "        conv_out = np.zeros(output.data.shape)\n",
    "\n",
    "        self.col_image = []\n",
    "        # do dot for every image in batch by im2col dot col_weight\n",
    "        for i in range(self.batch_size):\n",
    "            img_i = batch_img[i][np.newaxis, :]\n",
    "            col_image_i = im2col(img_i, self.ksize, self.stride)\n",
    "            conv_out[i] = np.reshape(np.dot(col_image_i, col_weights) + bias, output.data[0].shape)\n",
    "            self.col_image.append(col_image_i)\n",
    "        self.col_image = np.array(self.col_image)\n",
    "\n",
    "        output.data = conv_out\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层用到的 im2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, filter_height, filter_width, padding=0, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - filter_height) % stride == 0\n",
    "    assert (W + 2 * padding - filter_height) % stride == 0\n",
    "    \n",
    "    out_height = int((H + 2 * padding - filter_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - filter_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(filter_height), filter_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    \n",
    "    j0 = np.tile(np.arange(filter_width), filter_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    \n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), filter_height * filter_width).reshape(-1, 1)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, filter_height, filter_width, padding=0, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, filter_height, filter_width, padding,\n",
    "                                 stride=stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(filter_height * filter_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "# def im2col(image, ksize, stride):\n",
    "#     print(image.shape)\n",
    "#     N, H, W, C = image.shape\n",
    "#     image_temp = image.reshape(N, H, W, C)\n",
    "#     im2coled_image = im2col_indices(image_temp, ksize, ksize, padding=0, stride=stride)\n",
    "#     return im2coled_image.reshape((H - 2) * (W - 2), 25 * C)\n",
    "def im2col(image, ksize, stride):\n",
    "    image_col = []\n",
    "    # print(image.shape)\n",
    "    for i in range(0, image.shape[1] - ksize + 1, stride):\n",
    "        for j in range(0, image.shape[2] - ksize + 1, stride):\n",
    "            col = image[:, i:i + ksize, j:j + ksize, :].reshape([-1])\n",
    "\n",
    "            image_col.append(col)\n",
    "    image_col = np.array(image_col)\n",
    "    # print(image_col.shape)\n",
    "    return image_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大池化层\n",
    "```ksize``` 是卷积核大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(Operator):\n",
    "    def __init__(self, ksize=2, input_variable=Variable, name=str, stride=2):\n",
    "\n",
    "        if not isinstance(input_variable, Variable):\n",
    "            raise Exception(\"Operator Conv2D name: %s's input_variable is not instance of Variable\" % name)\n",
    "\n",
    "\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        self.batch_size = input_variable.shape[0]\n",
    "        self.output_channels = input_variable.shape[-1]\n",
    "        self.index = np.zeros(input_variable.shape)\n",
    "\n",
    "        self.input_variables = input_variable\n",
    "        _output_shape = [self.batch_size, input_variable.shape[2] / stride, input_variable.shape[2] / stride,\n",
    "                         self.output_channels]\n",
    "        self.output_variables = Variable(_output_shape, name='out', scope=name)\n",
    "        Operator.__init__(self, name, self.input_variables, self.output_variables)\n",
    "\n",
    "    def forward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            for parent in self.parent:\n",
    "                variables[parent].eval()\n",
    "            self._pool()\n",
    "            self.waiting_for_forward = False\n",
    "            return\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def backward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            pass\n",
    "        else:\n",
    "            for child in self.child:\n",
    "                variables[child].backward()\n",
    "            self.input_variables.grad = np.repeat(np.repeat(self.output_variables.grad, self.stride, axis=1),\n",
    "                                                  self.stride, axis=2) * self.index\n",
    "            self.waiting_for_forward = True\n",
    "            return\n",
    "\n",
    "    def _pool(self):\n",
    "        _out = np.zeros(self.output_variables.shape)\n",
    "        for b in range(self.input_variables.shape[0]):\n",
    "            for c in range(self.output_channels):\n",
    "                for i in range(0, self.input_variables.shape[1], self.stride):\n",
    "                    for j in range(0, self.input_variables.shape[2], self.stride):\n",
    "                        _out[b, i // self.stride, j // self.stride, c] = np.max(\n",
    "                            self.input_variables.data[b, i:i + self.ksize, j:j + self.ksize, c])\n",
    "                        index = np.argmax(self.input_variables.data[b, i:i + self.ksize, j:j + self.ksize, c])\n",
    "                        self.index[b, i+index // self.stride, j + index % self.stride, c] = 1\n",
    "        self.output_variables.data = _out\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullConnection(Operator):\n",
    "    def __init__(self, output_num, input_variable=Variable, name=str):\n",
    "        if not isinstance(input_variable, Variable):\n",
    "            raise Exception(\"Operator Conv2D name: %s's input_variable is not instance of Variable\" % name)\n",
    "\n",
    "        self.batch_size = input_variable.shape[0]\n",
    "        input_len = reduce(lambda x, y: x * y, input_variable.shape[1:])\n",
    "        self.output_num = output_num\n",
    "        self.weights = Variable([input_len, self.output_num], name='weights', scope=name, init='const' ,learnable=True)\n",
    "        self.bias = Variable([self.output_num], name='bias', scope=name, init='const',learnable=True)\n",
    "\n",
    "        self.output_variables = Variable([self.batch_size, self.output_num], name='out', scope=name)\n",
    "        self.input_variables = input_variable\n",
    "        Operator.__init__(self, name, self.input_variables, self.output_variables)\n",
    "\n",
    "    def forward(self):\n",
    "        if self.waiting_for_forward:\n",
    "\n",
    "            for parent in self.parent:\n",
    "                variables[parent].eval()\n",
    "            self.flatten_x = self.input_variables.data.reshape([self.batch_size, -1])\n",
    "            self.output_variables.data = np.dot(self.flatten_x, self.weights.data)+self.bias.data\n",
    "            self.waiting_for_forward = False\n",
    "            return\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def backward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            pass\n",
    "        else:\n",
    "            for child in self.child:\n",
    "                variables[child].backward()\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                col_x = self.flatten_x[i][:, np.newaxis]\n",
    "                diff_i = self.output_variables.grad[i][:, np.newaxis].T\n",
    "                self.weights.grad += np.dot(col_x, diff_i)\n",
    "                self.bias.grad += diff_i.reshape(self.bias.shape)\n",
    "            next_grad = np.dot(self.output_variables.grad, self.weights.data.T)\n",
    "            self.input_variables.grad = np.reshape(next_grad, self.input_variables.shape)\n",
    "\n",
    "            self.waiting_for_forward = True\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLoss(Operator):\n",
    "    def __init__(self, predict=Variable, label=Variable, name=str):\n",
    "        self.batch_size = predict.shape[0]\n",
    "        self.input_variables = [predict, label]\n",
    "        self.loss = Variable([1], name='loss', scope=name, init='None')\n",
    "        self.prediction = Variable(predict.shape, name='prediction', scope=name)\n",
    "        self.softmax = np.zeros(self.prediction.shape)\n",
    "\n",
    "        self.output_variables = [self.loss, self.prediction]\n",
    "        Operator.__init__(self, name, self.input_variables, self.output_variables)\n",
    "\n",
    "    def forward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            for parent in self.parent:\n",
    "                variables[parent].eval()\n",
    "\n",
    "            predict = self.input_variables[0].data\n",
    "            label = self.input_variables[1].data\n",
    "\n",
    "            self.prediction.data = self.predict(predict)\n",
    "            self.loss.data = 0\n",
    "            for i in range(self.batch_size):\n",
    "                self.loss.data += np.log(np.sum(np.exp(predict[i]))) - predict[i, label[i]]\n",
    "            self.loss.data /= self.batch_size\n",
    "            \n",
    "            self.waiting_for_forward = False\n",
    "            return\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            pass\n",
    "        else:\n",
    "            for child in self.child:\n",
    "                variables[child].backward()\n",
    "            self.input_variables[0].grad = self.softmax.copy()\n",
    "            for i in range(self.batch_size):\n",
    "                self.input_variables[0].grad[i, self.input_variables[1].data[i]] -= 1\n",
    "            self.waiting_for_forward = True\n",
    "            return\n",
    "\n",
    "    def predict(self, prediction):\n",
    "        exp_prediction = np.zeros(prediction.shape)\n",
    "        self.softmax = np.zeros(prediction.shape)\n",
    "        for i in range(self.batch_size):\n",
    "            prediction[i, :] -= np.max(prediction[i, :])\n",
    "            exp_prediction[i] = np.exp(prediction[i])\n",
    "            self.softmax[i] = exp_prediction[i]/np.sum(exp_prediction[i])\n",
    "        return self.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注册图\n",
    "根据 ```input_variable``` ```output_variable``` 和对应的 ```operator``` 在对应的 ```Variable``` 类实例中注册子节点和父节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_graph(input_variable, output_variable, operator):\n",
    "    if isinstance(input_variable,Variable) and isinstance(output_variable, Variable):\n",
    "        input_variable.child.append(operator.name)\n",
    "        output_variable.parent.append(operator.name)\n",
    "        operator.parent.append(input_variable.name)\n",
    "        operator.child.append(output_variable.name)\n",
    "\n",
    "    elif isinstance(input_variable, Variable) and len(output_variable)>1:\n",
    "        for output in output_variable:\n",
    "            input_variable.child.append(operator.name)\n",
    "            output.parent.append(operator.name)\n",
    "            operator.parent.append(input_variable.name)\n",
    "            operator.child.append(output.name)\n",
    "\n",
    "    elif isinstance(output_variable, Variable) and len(input_variable)>1:\n",
    "        for _input in input_variable:\n",
    "            _input.child.append(operator.name)\n",
    "            output_variable.parent.append(operator.name)\n",
    "            operator.parent.append(_input.name)\n",
    "            operator.child.append(output_variable.name)\n",
    "\n",
    "    elif len(output_variable)> 1 and len(input_variable)> 1:\n",
    "        for _input in input_variable:\n",
    "            _input.child.append(operator.name)\n",
    "            operator.parent.append(_input.name)\n",
    "        for output in output_variable:\n",
    "            output.parent.append(operator.name)\n",
    "            operator.child.append(output.name)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Operator name %s input,output list error'% operator.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数（ReLU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Operator):\n",
    "    def __init__(self, input_variable=Variable, name=str):\n",
    "        self.input_variables = input_variable\n",
    "        self.output_variables = Variable(self.input_variables.shape, name='out', scope=name)\n",
    "        Operator.__init__(self, name, self.input_variables, self.output_variables)\n",
    "\n",
    "    def forward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            for parent in self.parent:\n",
    "                variables[parent].eval()\n",
    "            self.output_variables.data = np.maximum(self.input_variables.data, 0)\n",
    "            self.waiting_for_forward = False\n",
    "            return\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def backward(self):\n",
    "        if self.waiting_for_forward:\n",
    "            pass\n",
    "        else:\n",
    "            for child in self.child:\n",
    "                variables[child].backward()\n",
    "            self.input_variables.grad = self.output_variables.grad\n",
    "            self.output_variables.grad[self.input_variables.data < 0] = 0\n",
    "            self.waiting_for_forward = True\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('mnist.npz')\n",
    "X, y, X_val, y_val, X_test, y_test = data['X'], data['y'], data['X_val'], data['y_val'], data['X_test'], data['y_test']\n",
    "# preprocessing\n",
    "# original: [0, 255]\n",
    "# after: [0, 1]\n",
    "X /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "# reshape the image data to NHWC format\n",
    "X = X.reshape(-1, 28, 28, 1)\n",
    "X_val = X_val.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pcc16214c4e)\">\r\n    <image height=\"218\" id=\"image8d4da631bb\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAABaJJREFUeJzt3b9rU30YxuFEO4ngIJTiJA5FUAQRhIIgFH+tTrp0cnMpiqNDK4ggOAjqIIIuSkFxkdqhDk4FQYro4D8hKForSLXv5CCYJ69pcqcx17XenOSAfPxCD0majUZjvQH01JZ+3wAMA6FBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBIz0+wb65fLly+U+MzOzodd/+/Zty21+fr68dmlpqdwXFhY6uif6x4kGAUKDAKFBgNAgQGgQIDQIEBoEDO1ztAMHDmzo+m/fvpX7vn37evbet27dKvcvX76U+82bN1tuHz586OieqDnRIEBoECA0CBAaBAgNAoQGAUKDgGaj0Vjv9030w9jYWLk/fPiw3O/evVvuP378aLkdPXq0vPbMmTPlvnPnznJfX6//ST9//txyu337dnnttWvXyn11dbXch5UTDQKEBgFCgwChQYDQIEBoECA0CBja52ib2d69e8t9bm6u3Pfv39/N2/nNo0ePyv38+fPlvrKy0s3bGRhONAgQGgQIDQKEBgFCgwChQYDQIMBztAE0MlJ/Hefp06fLvfpex9HR0Y7u6Zd2n2ebnp7e0OsPKicaBAgNAoQGAUKDAKFBgNAgYGh/tmmQra2tlfvjx4/LvfpppsXFxY7u6Zc9e/Zs6Pp/lRMNAoQGAUKDAKFBgNAgQGgQIDQI8BxtCG3fvr3ftzB0nGgQIDQIEBoECA0ChAYBQoMAoUGA52gD6NChQ+U+MTFR7leuXOnm7fzm3bt3PXvtQeZEgwChQYDQIEBoECA0CBAaBAgNAvxsU49MTU213CYnJ8trjx07Vu47duwo923btpX7Rty5c6fcL126VO7fv3/v5u0MDCcaBAgNAoQGAUKDAKFBgNAgwMdkOnT9+vVyv3jxYsut2Wxu6L23bKn/f/z582e5f/z4seV26tSp8to3b96Ue7uflBpWTjQIEBoECA0ChAYBQoMAoUGA0CDAx2RaGB8fL/eXL1+W++joaBfv5ncLCwvlPjs7W+6vX7/u5u3wPzjRIEBoECA0CBAaBAgNAoQGAUKDAJ9Ha6Hd16J9+vSp3Hv5HO3s2bPl/vXr1569N51xokGA0CBAaBAgNAgQGgQIDQKEBgE+j9ah3bt3l/vc3FzL7eDBg+W1W7duLffl5eVyb/edk0+ePCl3us+JBgFCgwChQYDQIEBoECA0CBAaBHiO1gczMzPlfu7cuXLftWtXua+urpb7vXv3Wm4XLlwor6UzTjQIEBoECA0ChAYBQoMAoUGAP+9vQlNTU+X+4MGDcl9fr/9Jqz//379/v7x2enq63PkzJxoECA0ChAYBQoMAoUGA0CBAaBDgOdoAOnHiRLk/f/6849du95NP7d771atXHb/3v8yJBgFCgwChQYDQIEBoECA0CBAaBIz0+wY2q7GxsXI/cuRIz957YmKi3CcnJ3v23i9evCj39+/f9+y9/2VONAgQGgQIDQKEBgFCgwChQYDQIGBoP492/Pjxcp+dnS33w4cPd/N2/kqz2Sz3dt/r+OzZs5Zbu++UXFlZKXf+zIkGAUKDAKFBgNAgQGgQIDQIEBoEDO1ztPn5+XI/efJk6E7+3traWrnfuHGj3K9evdpyq347jc450SBAaBAgNAgQGgQIDQKEBgFD++f98fHxcl9cXCz3dh8Xefr0actteXm5vLbdTyctLS2Vu4+ybD5ONAgQGgQIDQKEBgFCgwChQYDQIGBon6NBkhMNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBDwH0xB3/OcteorAAAAAElFTkSuQmCC\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m1c86d3a5c0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m1c86d3a5c0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m1c86d3a5c0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m1c86d3a5c0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m1c86d3a5c0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m1c86d3a5c0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m1c86d3a5c0\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"meb811f2640\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#meb811f2640\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#meb811f2640\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#meb811f2640\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#meb811f2640\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#meb811f2640\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#meb811f2640\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pcc16214c4e\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(X[1, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成 mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini batches\n",
    "# feed 125 examples to network at one time\n",
    "batch_size = 125\n",
    "# split mini batches\n",
    "data_length = X.shape[0]\n",
    "X_mini_batches = [X[i:i + batch_size] for i in range(0, data_length, batch_size)]\n",
    "y_mini_batches = [y[i:i + batch_size] for i in range(0, data_length, batch_size)]\n",
    "X_test_mini_batches = [X_test[i:i + batch_size] for i in range(0, len(X_test), batch_size)]\n",
    "y_test_mini_batches = [y_test[i:i + batch_size] for i in range(0, len(y_test), batch_size)]\n",
    "X_val_mini_batches = [X_val[i:i + batch_size] for i in range(0, len(X_val), batch_size)]\n",
    "y_val_mini_batches = [y_val[i:i + batch_size] for i in range(0, len(y_val), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络前传操作(图)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_forward(input, output_num):\n",
    "    conv1_out = Conv2D((5, 5, 1, 12), input_variable=input, name='conv1', padding='VALID').output_variables\n",
    "    relu1_out = Relu(input_variable=conv1_out, name='relu1').output_variables\n",
    "    #dropout1_out = op.DropOut(input_variable=relu1_out, name='dropout1', phase='train', prob=0.7).output_variables\n",
    "    pool1_out = MaxPooling(ksize=2, input_variable=relu1_out, name='pool1').output_variables\n",
    "\n",
    "    conv2_out = Conv2D((3, 3, 12, 24), input_variable=pool1_out, name='conv2').output_variables\n",
    "    relu2_out = Relu(input_variable=conv2_out, name='relu2').output_variables\n",
    "    #dropout2_out = op.DropOut(input_variable=relu2_out, name='dropout2', phase='train', prob=0.7).output_variables\n",
    "    pool2_out = MaxPooling(ksize=2, input_variable=relu1_out, name='pool2').output_variables\n",
    "\n",
    "    fc_out = FullConnection(output_num=output_num, input_variable=pool2_out, name='fc').output_variables\n",
    "    return fc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义给每次训练 mini_batch 时候用的 data 和 label 槽位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_placeholder = Variable((batch_size, 28, 28, 1), name='data')\n",
    "label_placeholder = Variable([batch_size, 1], name='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义 Softmax 分类器\n",
    "\n",
    "将前传用到的 ```img_placeholder``` 槽与输出绑定。\n",
    "\n",
    "将输出与分类器和正确 label ```label_placeholder``` 槽绑定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = network_forward(img_placeholder, 10)\n",
    "sf = SoftmaxLoss(prediction, label_placeholder, 'sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d945b1d8df7440d59cc3b4482f2740ea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=480), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6293a0be0164fb3baad704bf87bbab4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index 0 in iteration 0: loss: 2.5645669342693265, acc: 7.199999999999999%\nIndex 25 in iteration 0: loss: 0.8613034931148064, acc: 76.8%\nIndex 50 in iteration 0: loss: 0.4647399818086439, acc: 85.6%\nIndex 75 in iteration 0: loss: 0.4397584107314884, acc: 88.8%\nIndex 100 in iteration 0: loss: 0.38444288033630186, acc: 87.2%\nIndex 125 in iteration 0: loss: 0.3148070990532096, acc: 92.0%\nIndex 150 in iteration 0: loss: 0.36907060663999847, acc: 87.2%\nIndex 175 in iteration 0: loss: 0.18908156432415285, acc: 95.19999999999999%\nIndex 200 in iteration 0: loss: 0.2651221763224927, acc: 90.4%\nIndex 225 in iteration 0: loss: 0.1414590804370672, acc: 96.8%\nIndex 250 in iteration 0: loss: 0.2176724822648768, acc: 93.60000000000001%\nIndex 275 in iteration 0: loss: 0.1621818828123404, acc: 96.0%\nIndex 300 in iteration 0: loss: 0.23892980759925025, acc: 92.0%\nIndex 325 in iteration 0: loss: 0.18415239947257947, acc: 92.80000000000001%\nIndex 350 in iteration 0: loss: 0.21618966970342973, acc: 94.39999999999999%\nIndex 375 in iteration 0: loss: 0.3951719883259543, acc: 88.8%\nIndex 400 in iteration 0: loss: 0.1474516664636491, acc: 96.8%\nIndex 425 in iteration 0: loss: 0.11982279223698553, acc: 96.0%\nIndex 450 in iteration 0: loss: 0.215517748879853, acc: 94.39999999999999%\nIndex 475 in iteration 0: loss: 0.15067892894628468, acc: 95.19999999999999%\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=480), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3ef0d3994804a3daf539f710937fe01"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index 0 in iteration 1: loss: 0.15242820294153023, acc: 96.0%\nIndex 25 in iteration 1: loss: 0.36508885625746695, acc: 90.4%\nIndex 50 in iteration 1: loss: 0.1841960253768886, acc: 95.19999999999999%\nIndex 75 in iteration 1: loss: 0.15141267103873607, acc: 96.8%\nIndex 100 in iteration 1: loss: 0.2157384576164023, acc: 93.60000000000001%\nIndex 125 in iteration 1: loss: 0.14190908977904731, acc: 96.0%\nIndex 150 in iteration 1: loss: 0.15205440143198237, acc: 96.0%\nIndex 175 in iteration 1: loss: 0.08622174238447002, acc: 97.6%\nIndex 200 in iteration 1: loss: 0.11449752525297222, acc: 96.8%\nIndex 225 in iteration 1: loss: 0.0645898680155513, acc: 98.4%\nIndex 250 in iteration 1: loss: 0.12550936857645462, acc: 96.0%\nIndex 275 in iteration 1: loss: 0.10304830401670458, acc: 97.6%\nIndex 300 in iteration 1: loss: 0.12904788610244294, acc: 96.0%\nIndex 325 in iteration 1: loss: 0.10882412229895197, acc: 96.0%\nIndex 350 in iteration 1: loss: 0.1339001327294398, acc: 96.8%\nIndex 375 in iteration 1: loss: 0.29054281827809536, acc: 90.4%\nIndex 400 in iteration 1: loss: 0.08329008245221399, acc: 99.2%\nIndex 425 in iteration 1: loss: 0.06260920421136834, acc: 99.2%\nIndex 450 in iteration 1: loss: 0.1595053493832024, acc: 96.8%\nIndex 475 in iteration 1: loss: 0.11436630088898991, acc: 96.8%\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=480), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6288a04fe95a4a9cb3a5a45ac6e30c52"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index 0 in iteration 2: loss: 0.10140977715067415, acc: 97.6%\nIndex 25 in iteration 2: loss: 0.2899413995007922, acc: 91.2%\nIndex 50 in iteration 2: loss: 0.14185833412827595, acc: 96.8%\nIndex 75 in iteration 2: loss: 0.10425547695544211, acc: 96.8%\nIndex 100 in iteration 2: loss: 0.17564200839012822, acc: 95.19999999999999%\nIndex 125 in iteration 2: loss: 0.10605765686400061, acc: 96.8%\nIndex 150 in iteration 2: loss: 0.1148678901841676, acc: 97.6%\nIndex 175 in iteration 2: loss: 0.05931791493264125, acc: 100.0%\nIndex 200 in iteration 2: loss: 0.09082800731053554, acc: 96.8%\nIndex 225 in iteration 2: loss: 0.04959185652493264, acc: 99.2%\nIndex 250 in iteration 2: loss: 0.10045627366572142, acc: 98.4%\nIndex 275 in iteration 2: loss: 0.08711558461231249, acc: 98.4%\nIndex 300 in iteration 2: loss: 0.10246319821386145, acc: 97.6%\nIndex 325 in iteration 2: loss: 0.08287514755493461, acc: 98.4%\nIndex 350 in iteration 2: loss: 0.10746644619013984, acc: 96.8%\nIndex 375 in iteration 2: loss: 0.24435232531425033, acc: 92.0%\nIndex 400 in iteration 2: loss: 0.06587495698889843, acc: 99.2%\nIndex 425 in iteration 2: loss: 0.05021730107959878, acc: 100.0%\nIndex 450 in iteration 2: loss: 0.1386738359390977, acc: 97.6%\nIndex 475 in iteration 2: loss: 0.09401816337483457, acc: 96.8%\n\n\n"
    }
   ],
   "source": [
    "iterate_count = 3\n",
    "for i in notebook.tqdm_notebook(range(iterate_count)):\n",
    "    # iterate through all datasets\n",
    "    for idx, (X_batch, y_batch) in notebook.tqdm_notebook(enumerate(zip(X_mini_batches, y_mini_batches)), total = len(X_mini_batches)):\n",
    "        img_placeholder.data = X_batch\n",
    "        label_placeholder.data = y_batch.astype(np.int)\n",
    "\n",
    "        # forward\n",
    "        loss_value = sf.loss.eval()\n",
    "        pred = sf.prediction.eval()\n",
    "        acc = np.mean(np.argmax(pred, axis=1) == y_batch)\n",
    "\n",
    "        acc_str = str(100 * acc) + \"%\"\n",
    "\n",
    "        if idx % 25 == 0:\n",
    "            print(\"Index \" + str(idx) + \" in iteration \" + str(i) + \": loss: \" + str(loss_value) + \", acc: \" + acc_str)\n",
    "            # print(pred)\n",
    "            # print(y_batch)\n",
    "            # print(np.argmax(pred, axis=1) == y_batch)\n",
    "            \n",
    "        # backward\n",
    "        dimg = img_placeholder.backward()\n",
    "        # apply gradient\n",
    "        for variable_name in variables:\n",
    "            variable_instance = variables[variable_name]\n",
    "            if isinstance(variable_instance, Variable) and variable_instance.learnable:\n",
    "                variable_instance.apply_gradient(learning_rate=8e-2, decay_rate=3e-4, batch_size=batch_size)\n",
    "            if isinstance(variable_instance, Variable):\n",
    "                variable_instance.grad = np.zeros(variable_instance.shape)\n",
    "        # if (acc > 0.8):\n",
    "        #     print(\"Now acc > 0.8. Break.\")\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存/读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('variables.pickle', 'wb') as handle:\n",
    "    pickle.dump(variables, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_batches, label_batches, test_object_desc_str):\n",
    "    correct_count = 0\n",
    "    iter_count = 0\n",
    "    for idx, (X_batch, y_batch) in notebook.tqdm_notebook(enumerate(zip(data_batches, label_batches)),\n",
    "                                total = len(data_batches),\n",
    "                                leave = False):\n",
    "            # if (idx != 4):\n",
    "            #     continue\n",
    "            img_placeholder.data = X_batch\n",
    "            label_placeholder.data = y_batch.astype(np.int)\n",
    "\n",
    "            for variable_name in variables:\n",
    "                variable_instance = variables[variable_name]\n",
    "                if isinstance(variable_instance, Variable):\n",
    "                    variable_instance.waiting_for_backprop = False\n",
    "                if isinstance(s, Operator):\n",
    "                    variable_instance.waiting_for_forward = True\n",
    "                if isinstance(s, Operator) and hasattr(s,'phase'):\n",
    "                        variable_instance.phase = 'test'\n",
    "\n",
    "            # forward\n",
    "            loss_value = sf.loss.eval()\n",
    "            pred = sf.prediction.eval()\n",
    "            correct_count += np.sum(np.argmax(pred, axis=1) == y_batch)\n",
    "            iter_count = iter_count + 1\n",
    "    print(\"Correct / All: \", correct_count, '/', iter_count * 125)\n",
    "    print(test_object_desc_str + ' set accuracy: ' + str(correct_count/(iter_count * 125) * 100) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=480), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e449d0297d0413c9787d6bf061a41aa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Correct / All:57955/60000\nTraining set accuracy: 96.59166666666667%\n"
    }
   ],
   "source": [
    "validate(X_mini_batches, y_mini_batches, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e0bf4b593d449fd80f0596fcc2ca636"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Correct / All:4823/5000\nTest set accuracy: 96.46000000000001%\n"
    }
   ],
   "source": [
    "validate(X_test_mini_batches, y_test_mini_batches, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56b26a2a87294e79be01c1d5631a16b8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Correct / All:4839/5000\nVal set accuracy: 96.78%\n"
    }
   ],
   "source": [
    "validate(X_val_mini_batches, y_val_mini_batches, \"Val\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}